# -*- coding: utf-8 -*-
"""7_AARA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iy4VnHY01UZ_uSNpiCn3F5_iujG2qww9
"""

import pandas as pd
import numpy as np
import pickle
from tqdm.auto import tqdm
import networkx as nx
from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, accuracy_score
from sklearn.preprocessing import MinMaxScaler
from numpy.random import RandomState

# load edges (or links) from edge-list
with open("./data/fb-pages-food.edges", encoding="utf8") as f:
    fb_links = f.read().splitlines()

len(fb_links)

node_list_1 = []
node_list_2 = []

for i in tqdm(fb_links):
    node_1, node_2 = i.split(',')[0], i.split(',')[1]
    if node_1 != node_2:
        node_list_1.append(node_1)
        node_list_2.append(node_2)

fb_df1 = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})

G = nx.from_pandas_edgelist(fb_df1, "node_1", "node_2", create_using=nx.Graph())

print(nx.info(G))

node_list = list(G.nodes)

# Build adjacency matrix from original graph
adj_G = nx.to_numpy_matrix(G, nodelist = G.nodes)
adj_G.shape

total_edges = G.number_of_edges()
thirty_percent_total_edges = int(0.3 * total_edges)
thirty_percent_total_edges

average_degree = (2*total_edges) // G.number_of_nodes()
average_degree_half = average_degree // 2
average_degree_half

initial_node_count = len(G.nodes)
fb_df_temp = fb_df1.copy()
# empty list to store removable links
omissible_links_index = []
random_state_ = 100
for i in tqdm(range(len(fb_df1))):
    # selecting a random edge
    random_state_ += 1
    random_row_index = fb_df_temp.sample(random_state=random_state_).index
    # remove a node pair and build a new graph
    G_temp = nx.from_pandas_edgelist(fb_df_temp.drop(index = random_row_index), "node_1", "node_2", create_using=nx.Graph())
    average_degree_temp = (2*G_temp.number_of_edges()) // G_temp.number_of_nodes()
    # check there is no spliting of graph, number of nodes is same,
    # and average degree should not lower than the average_degree/2
    if (nx.number_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):
        omissible_links_index.append(random_row_index.values[0])
        fb_df_temp = fb_df_temp.drop(index = random_row_index)
        if (average_degree_temp < average_degree_half) or \
        (len(omissible_links_index) >= thirty_percent_total_edges):
            break

len(omissible_links_index)

# create dataframe of removable edges
fb_df_ghost = fb_df1.loc[omissible_links_index]

# drop removable edges
fb_df_partial = fb_df1.drop(index=fb_df_ghost.index.values)
# build graph G_data that will be used in score calculation
G_data = nx.from_pandas_edgelist(fb_df_partial, "node_1", "node_2", create_using=nx.Graph())

print(nx.info(G_data))

path = dict(nx.all_pairs_shortest_path(G_data,cutoff=2))

# Generating negative samples or finding zeros in adjacency matrix
all_unconnected_pairs = []
offset = 0
for i in tqdm(range(adj_G.shape[0])):
    for j in range(offset,adj_G.shape[1]):
        if i != j and adj_G[i,j] == 0:
            all_unconnected_pairs.append([node_list[i],node_list[j]])
            try:
                if len(path[str(i)][str(j)]) - 1 <= 2:
                    all_unconnected_pairs.append([node_list[i],node_list[j]])
            except:
                pass
    offset = offset + 1

print(len(all_unconnected_pairs))

#Create dataframe for negative samples
node_1_unlinked = [i[0] for i in all_unconnected_pairs]
node_2_unlinked = [i[1] for i in all_unconnected_pairs]
data = pd.DataFrame({'node_1':node_1_unlinked,
                     'node_2':node_2_unlinked})
# add target variable 'link'
data['link'] = 0

#Randomly selecting thirty_percent_total_edges no. of
#data points from negative samples so that resultant dataset is balanced
data = data.sample(n=len(omissible_links_index), random_state=101)
data = data.reset_index(drop=True)
len(data)

# add the target variable 'link' in dataframe formed from removing edges
fb_df_ghost['link'] = 1
# add the removed edges into the data
#pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
data = pd.concat([data,fb_df_ghost[['node_1', 'node_2', 'link']]], ignore_index=True)

data['link'].value_counts()

def psim(G, sortest_path, eta, mu, node1, node2):
    sim_score = eta*list(nx.adamic_adar_index(G, [(node1, node2)]))[0][2] + mu*list(nx.resource_allocation_index(G, [(node1, node2)]))[0][2] + (1-(eta+mu))*(len(G)/(len(sortest_path[node1][node2])-1))
    return (node1, node2, sim_score)

from time import time
from statistics import mean
time_ls = []
for i in range(2):
    start = time()
    G = G_data.copy()
    path = dict(nx.all_pairs_shortest_path(G))
    pred = []
    node_1 = []
    node_2 = []
    for i in data.itertuples():
        #pred.append(sim(G_data, i[1], i[2]))
        pred.append(psim(G, path, 0.4, 0.4, i[1],i[2])+(i[3],))
    end = time()
    time_ls.append(end-start)
print('Average Time: ', mean(time_ls), end='\n')

data_final = pd.DataFrame(pred, columns =['node_1', 'node_2', 'Score', 'link'])
len(data_final)

# with open('FriendTNS-fb-pages-politician.pickle', 'wb') as f:
#     pickle.dump(data_final, f, protocol=pickle.HIGHEST_PROTOCOL)

data_final.boxplot(column=['Score'])

max_score = max(data_final['Score'])
min_score = min(data_final['Score'])
print("Max Score: "+str(max(data_final['Score'])))
print("\nMin Score: "+str(min(data_final['Score'])))

if max_score <= 1 and min_score >= 0:
    print("Do nothing")

else:
    # create a scaler object
    scaler = MinMaxScaler()
    # fit and transform the data
    data_final['Score'] = scaler.fit_transform(data_final[['Score']])

auc = round(roc_auc_score(data_final['link'], data_final['Score']),4)
print("AUC: ",auc)

fpr, tpr, threshold = roc_curve(data_final['link'], data_final['Score'])
roc_df = pd.DataFrame()
roc_df['fpr'] = fpr
roc_df['tpr'] = tpr
roc_df['threshold'] = threshold

data_final['Score'].mean()

#y_pred = data_final['Score'] > 0.6       #fb-pages-politician.edges
#y_pred = data_final['Score'] > 0.001      #facebook_ego
#y_pred = data_final['Score'] > 0.9     #SS-Butterfly
#y_pred = data_final['Score'] > 0.55     #fb-pages-food
#y_pred = data_final['Score'] > 0.01      #fb-pages-food FriendTNS
#y_pred = data_final['Score'] > 0.5      #small datasets

y_pred = data_final['Score'] >= data_final['Score'].mean()

precision = round(precision_score(data_final['link'], y_pred),4)
print("Precision: ",precision)
recall = round(recall_score(data_final['link'], y_pred),4)
print("\nRecall: ",recall)
accuracy = round(accuracy_score(data_final['link'], y_pred),4)
print("\nAccuracy: ",accuracy)
f_measure = round((2*precision*recall)/(precision + recall),4)
print("\nf_measure: ",f_measure)